{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import td_prediction, mc_control_on_policy, sarsa, q_learning, exp_sarsa, nstep_sarsa\n",
    "from env import get_random_walk_env, get_windy_gridworld_env\n",
    "from policy import get_equiprobable_policy\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_BASE_PATH = './figs'\n",
    "if os.path.exists(FIG_BASE_PATH) is False:\n",
    "    os.makedirs(FIG_BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk (7.2 RL2e Example)\n",
    "\n",
    "Comparing n-step TD methods with varying n and $\\alpha$ values. Goal is to understand how the choice of n and $\\alpha$ affects the performance of the algorithm. I also compare different terminal rewards and number of states in the Random Walk environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(72)\n",
    "NUM_STATES = 21\n",
    "env_21_negative_left = get_random_walk_env(num_states=NUM_STATES, rewards=[-1,1])\n",
    "env_21_zero_left = get_random_walk_env(num_states=NUM_STATES, rewards=[0,1])\n",
    "env_7_states = get_random_walk_env(num_states=7, rewards=[0,1])\n",
    "\n",
    "def gen_episode_random_policy(env, num_episodes=10):\n",
    "    policy = get_equiprobable_policy(env)\n",
    "    episodes = []\n",
    "    for i in range(num_episodes):\n",
    "        episode = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            action, _ = policy(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "    \n",
    "episode_env_21_negative_left = gen_episode_random_policy(env_21_negative_left)\n",
    "episode_env_21_zero_left = gen_episode_random_policy(env_21_zero_left)\n",
    "episode_env_7_states = gen_episode_random_policy(env_7_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(v, v_hat):\n",
    "    return ((v - v_hat) ** 2).mean() ** 0.5\n",
    "\n",
    "true_V_21 = np.arange(-9, 10) / 10 # derived using Bellman equation on equiprobable policy\n",
    "true_V_7 = np.arange(1, 6) / 6 # derived using Bellman equation on equiprobable policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEP_VALUES = [1,2,4,8,16,32,64,128,256,512]\n",
    "ALPHAS = np.arange(0, 1.01, 0.01)\n",
    "NUM_EXPERIMENTS = 100\n",
    "\n",
    "def run_experiment(env, episodes, true_V, n_step_values=N_STEP_VALUES, alphas=ALPHAS, num_experiments=NUM_EXPERIMENTS, title=None, save_path=None):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    if save_path is not None and os.path.exists(save_path):\n",
    "        # load from file if exists and put in imshow and skip computation\n",
    "        # assume save_path is a .png file\n",
    "        img = plt.imread(save_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        for n in tqdm.tqdm(n_step_values):\n",
    "            v_over_time = np.zeros((len(alphas), num_experiments, env.observation_space.n))\n",
    "            for i, alpha in enumerate(alphas):\n",
    "                for j in range(num_experiments):\n",
    "                    # get dict of V and turn it into ndarray\n",
    "                    V = td_prediction(env, 1, episodes, alpha, n=n)\n",
    "                    V_np = np.zeros(env.observation_space.n)\n",
    "                    for k, v in V.items():\n",
    "                        V_np[k] = v\n",
    "                    v_over_time[i, j] = V_np\n",
    "            rmse_per_experiment = np.apply_along_axis(lambda x: rmse(x[1:env.unwrapped.num_states-1], true_V), 2, v_over_time)\n",
    "            average_rmse = rmse_per_experiment.mean(axis=1)\n",
    "            plt.plot(alphas, average_rmse, label=f'n={n}')\n",
    "\n",
    "        plt.xlabel('$\\\\alpha$')\n",
    "        plt.ylabel('Average RMS error over states and first 10 episodes')\n",
    "        \n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "        plt.legend()\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_21_STATE_RANDOM_WALK_NEGATIVE_LEFT_PATH = os.path.join(FIG_BASE_PATH, '21_state_random_walk_negative_left.png')\n",
    "FIG_21_STATE_RANDOM_WALK_ZERO_LEFT_PATH = os.path.join(FIG_BASE_PATH, '21_state_random_walk_zero_left.png')\n",
    "FIG_7_STATE_RANDOM_WALK_PATH = os.path.join(FIG_BASE_PATH, '7_state_random_walk.png')\n",
    "\n",
    "run_experiment(env_21_negative_left, episode_env_21_negative_left, true_V_21, title='(19 State) Random walk with (-1,1) rewards', save_path=FIG_21_STATE_RANDOM_WALK_NEGATIVE_LEFT_PATH)\n",
    "run_experiment(env_21_zero_left, episode_env_21_zero_left, true_V_21, title='(19 State) Random walk with (0,1) rewards', save_path=FIG_21_STATE_RANDOM_WALK_ZERO_LEFT_PATH)\n",
    "run_experiment(env_7_states, episode_env_7_states, true_V_7, title='(7 State) Random walk with (0,1) rewards', save_path=FIG_7_STATE_RANDOM_WALK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_td_update_percentage(episodes, n_values=N_STEP_VALUES):\n",
    "    \"\"\"\n",
    "    For all n values, calculate parcentage of n-step updates that use bootstrapping vs monte carlo updates\n",
    "    \"\"\"\n",
    "    avg_percent_td_update = np.zeros((len(n_values), len(episodes)))\n",
    "    for i, n in enumerate(n_values):\n",
    "        for j, episode in enumerate(episodes):\n",
    "            td_update_count = 0   \n",
    "            T = len(episode)\n",
    "            for t in range(T):\n",
    "                if t + n < T:\n",
    "                    td_update_count += 1\n",
    "            avg_percent_td_update[i, j] = td_update_count / (T - 1)\n",
    "    \n",
    "    return avg_percent_td_update.mean(axis=1)\n",
    "\n",
    "avg_percent_td_update_21_state = calculate_td_update_percentage(episode_env_21_negative_left)\n",
    "avg_percent_td_update_7_state = calculate_td_update_percentage(episode_env_7_states)\n",
    "\n",
    "# create table of percentage of td updates that are monte carlo updates use pandas\n",
    "df_21_state = pd.DataFrame({'n': N_STEP_VALUES, 'percentage_td_updates': avg_percent_td_update_21_state})\n",
    "df_7_state = pd.DataFrame({'n': N_STEP_VALUES, 'percentage_td_updates': avg_percent_td_update_7_state})\n",
    "df_21_state['percentage_td_updates'] = df_21_state['percentage_td_updates'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_7_state['percentage_td_updates'] = df_7_state['percentage_td_updates'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "\n",
    "# print table without index\n",
    "print(\"21 State Random Walk with (-1,1) rewards\\n\", df_21_state.to_string(index=False))\n",
    "print(\"7 State Random Walk with (0,1) rewards\\n\", df_7_state.to_string(index=False))\n",
    "\n",
    "\n",
    "print(f'Episode length for 21 state random walk with (-1,1) rewards: {[len(episode) for episode in episode_env_21_negative_left]}')\n",
    "print(f'Episode length for 7 state random walk with (0,1) rewards: {[len(episode) for episode in episode_env_7_states]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windy Gridworld (6.5 RL2e Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windy_env = get_windy_gridworld_env()\n",
    "NUM_STEPS = 8000\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.1\n",
    "ALPHA = 0.5\n",
    "N_STEP = 4\n",
    "mc_Q, mc_episode_at_step = mc_control_on_policy(windy_env, NUM_STEPS, GAMMA, EPSILON, ALPHA, verbose=True)\n",
    "sarsa_Q, sarsa_episode_at_step = sarsa(windy_env, NUM_STEPS, GAMMA, EPSILON, ALPHA, verbose=True)\n",
    "exp_sarsa_Q, exp_sarsa_episode_at_step = exp_sarsa(windy_env, NUM_STEPS, GAMMA, EPSILON, ALPHA, verbose=True)\n",
    "q_learning_Q, q_learning_episode_at_step = q_learning(windy_env, NUM_STEPS, GAMMA, EPSILON, ALPHA, verbose=True)\n",
    "n_step_sarsa_Q, n_step_sarsa_episode_at_step = nstep_sarsa(windy_env, NUM_STEPS, GAMMA, EPSILON, ALPHA, N_STEP, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(mc_episode_at_step, label='MC Control')\n",
    "plt.plot(sarsa_episode_at_step, label='Sarsa')\n",
    "plt.plot(exp_sarsa_episode_at_step, label='Expected Sarsa')\n",
    "plt.plot(q_learning_episode_at_step, label='Q Learning')\n",
    "plt.plot(n_step_sarsa_episode_at_step, label=f'{N_STEP}-step Sarsa')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps per episode')\n",
    "plt.title('Windy Gridworld')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(FIG_BASE_PATH, 'windy_gridworld.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "School",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
