{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms import td_prediction\n",
    "from env import get_random_walk_env\n",
    "from policy import get_equiprobable_policy\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk (7.2 RL2e Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STATES = 21\n",
    "env = get_random_walk_env(num_states=NUM_STATES)\n",
    "policy = get_equiprobable_policy(env)\n",
    "episodes = []\n",
    "for i in range(10):\n",
    "    episode = []\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not done and not truncated:\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    episodes.append(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(v, v_hat):\n",
    "    return ((v - v_hat) ** 2).mean() ** 0.5\n",
    "\n",
    "true_V = np.arange(-9, 10) / 10 # derived using Bellman equation on equiprobable policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEP_VALUES = [1,2,4,8,16,32,64,128,256,512]\n",
    "ALPHAS = np.arange(0, 1.01, 0.01)\n",
    "NUM_EXPERIMENTS = 100\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for n in tqdm.tqdm(N_STEP_VALUES):\n",
    "    v_over_time = np.zeros((len(ALPHAS), NUM_EXPERIMENTS, env.observation_space.n))\n",
    "    for i, alpha in enumerate(ALPHAS):\n",
    "        for j in range(NUM_EXPERIMENTS):\n",
    "            # get dict of V and turn it into ndarray\n",
    "            V = td_prediction(env, 1, episodes, alpha, n=n)\n",
    "            V_np = np.zeros(env.observation_space.n)\n",
    "            for k, v in V.items():\n",
    "                V_np[k] = v\n",
    "            v_over_time[i, j] = V_np\n",
    "    rmse_per_experiment = np.apply_along_axis(lambda x: rmse(x[1:NUM_STATES-1], true_V), 2, v_over_time)\n",
    "    average_rmse = rmse_per_experiment.mean(axis=1)\n",
    "    plt.plot(ALPHAS, average_rmse, label=f'n={n}')\n",
    "\n",
    "plt.xlabel('$\\\\alpha$')\n",
    "plt.ylabel('Average RMS error over 19 states and first 10 episodes')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "School",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
